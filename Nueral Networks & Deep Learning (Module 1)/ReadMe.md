# Deep Learning Practice - Neural Networks & Deep Learning (Module 1)

This repository contains my practice implementations and notes for **Neural Networks & Deep Learning**, which is Module 1 of my deep learning journey. The code and exercises are based on foundational concepts of neural networks, forward and backward propagation, and basic deep learning architectures.

## 📂 Repository Structure

```
Nueral Networks & Deep Learning (Module 1)/
├── Week 1 - Introduction to Deep Learning/
│   ├── Python Basics with NumPy/
│   │   ├── Python_Basics_With_Numpy.ipynb
│   │   └── ...
│   └── Logistic Regression as a Neural Network/
│       ├── Logistic_Regression_with_a_Neural_Network_mindset.ipynb
│       └── ...
├── Week 2 - Neural Networks Basics/
│   ├── Python Vectorization/
│   │   ├── Vectorization.ipynb
│   │   └── ...
│   └── Logistic Regression Neural Network/
│       ├── Logistic_Regression_Neural_Network.ipynb
│       └── ...
├── Week 3 - Shallow Neural Networks/
│   ├── Shallow Neural Network Implementation/
│   │   ├── Shallow_neural_network.ipynb
│   │   └── ...
│   └── ...
├── Week 4 - Deep Neural Networks/
│   ├── Deep Neural Network - Application/
│   │   ├── Deep_Neural_Network_Application.ipynb
│   │   └── ...
│   └── ...
└── README.md
```

## 📌 Weekly Breakdown

### **Week 1: Introduction to Deep Learning**
- **Python Basics with NumPy**: Covers essential NumPy operations for deep learning.
- **Logistic Regression as a Neural Network**: Implements logistic regression using a neural network mindset.

#### Key Concepts:
- Basics of NumPy arrays and operations.
- Vectorization for efficient computation.
- Binary classification using logistic regression.
- Cost function and gradient descent.

---

### **Week 2: Neural Networks Basics**
- **Python Vectorization**: Optimizing operations using vectorization.
- **Logistic Regression Neural Network**: Extending logistic regression into a simple neural network.

#### Key Concepts:
- Forward and backward propagation.
- Derivatives and computation graphs.
- Gradient descent optimization.
- Broadcasting in Python.

---

### **Week 3: Shallow Neural Networks**
- **Shallow Neural Network Implementation**: Building a 1-hidden-layer neural network.

#### Key Concepts:
- Activation functions (Sigmoid, ReLU, Tanh).
- Random initialization of weights.
- Forward and backward propagation in a 2-layer NN.
- Hyperparameter tuning (learning rate, hidden units).

---

### **Week 4: Deep Neural Networks**
- **Deep Neural Network - Application**: Implementing an L-layer deep neural network for image classification.

#### Key Concepts:
- Deep vs. shallow networks.
- Forward and backward propagation in deep networks.
- Parameter initialization strategies.
- Hyperparameter selection in deep learning.

## 🛠️ Dependencies
- Python 3.7+
- NumPy
- Matplotlib (for visualization)
- Jupyter Notebook (for interactive coding)

## 🚀 How to Use This Repository
1. Clone the repo:
   ```bash
   git clone https://github.com/AhmedButtar7/Deep_Learning_Practice_Python.git
   ```
2. Navigate to the desired week and topic:
   ```bash
   cd Nueral Networks & Deep Learning (Module 1)/Week X - Topic/
   ```
3. Open and run the Jupyter Notebooks to follow along.

## 📝 Notes
- Each notebook contains detailed explanations, mathematical derivations, and code implementations.
- Exercises are based on Andrew Ng's Deep Learning Specialization (Coursera) with additional personal modifications.
- Some datasets (e.g., Cat vs. Non-Cat images) are preloaded in the notebooks.

## 🔗 References
- [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning)
- [NumPy Documentation](https://numpy.org/doc/)
- [Neural Networks and Deep Learning - Andrew Ng](http://neuralnetworksanddeeplearning.com/)

---

**Happy Learning!** 🚀  
**Ahmed Buttar**  
📧 [Email](mailto:ahmedbuttar7@gmail.com) | 🔗 [GitHub](https://github.com/AhmedButtar7) | 💼 [LinkedIn](https://linkedin.com/in/ahmedbuttar)
