# Deep Learning Practice - Improving Deep Neural Networks (Module 2)

This repository contains my implementations and notes for **Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization**, which is Module 2 of my deep learning journey. This module focuses on advanced techniques to optimize and improve the performance of deep neural networks.

## 📂 Repository Structure

```
Improving Deep Neural Networks Hyperparameter Tuning, Regularization and Optimization (Module 2)/
├── Week 1 - Practical Aspects of Deep Learning/
│   ├── Initialization/
│   │   ├── Initialization.ipynb
│   │   └── ...
│   ├── Regularization/
│   │   ├── Regularization.ipynb
│   │   └── ...
│   ├── Gradient Checking/
│   │   ├── Gradient_Checking.ipynb
│   │   └── ...
│   └── ...
├── Week 2 - Optimization Algorithms/
│   ├── Optimization Methods/
│   │   ├── Optimization_methods.ipynb
│   │   └── ...
│   └── ...
├── Week 3 - Hyperparameter Tuning, Batch Normalization and Programming Frameworks/
│   ├── TensorFlow Tutorial/
│   │   ├── TensorFlow_Tutorial.ipynb
│   │   └── ...
│   ├── Hyperparameter Tuning/
│   │   ├── Hyperparameter_tuning.ipynb
│   │   └── ...
│   └── ...
└── README.md
```

## 📌 Weekly Breakdown

### **Week 1: Practical Aspects of Deep Learning**
- **Initialization**: Comparing different weight initialization techniques and their impact on model convergence.
- **Regularization**: Implementing L2 regularization and dropout to prevent overfitting.
- **Gradient Checking**: Verifying the correctness of backpropagation implementation using numerical gradient methods.

#### Key Concepts:
- Importance of proper weight initialization (Zero, Random, He, Xavier)
- Regularization techniques (L2, Dropout)
- Gradient checking implementation and best practices
- Overfitting detection and prevention strategies

---

### **Week 2: Optimization Algorithms**
- **Optimization Methods**: Implementing various optimization algorithms beyond basic gradient descent.

#### Key Concepts:
- Mini-batch gradient descent
- Exponentially weighted averages
- Momentum optimization
- RMSProp
- Adam optimization algorithm
- Learning rate decay techniques

---

### **Week 3: Hyperparameter Tuning, Batch Normalization and Programming Frameworks**
- **TensorFlow Tutorial**: Introduction to TensorFlow for deep learning implementations.
- **Hyperparameter Tuning**: Systematic approaches for tuning hyperparameters.
- **Batch Normalization**: Implementing batch normalization to accelerate deep network training.

#### Key Concepts:
- TensorFlow basics and computational graphs
- Hyperparameter tuning strategies (random vs. grid search)
- Batch normalization implementation and benefits
- Multi-class classification with softmax regression
- Introduction to deep learning frameworks

## 🛠️ Dependencies
- Python 3.7+
- NumPy
- TensorFlow 2.x
- Matplotlib (for visualization)
- Jupyter Notebook
- scikit-learn (for some datasets and utilities)

## 🚀 How to Use This Repository
1. Clone the repo:
   ```bash
   git clone https://github.com/AhmedButtar7/Deep_Learning_Practice_Python.git
   ```
2. Navigate to Module 2:
   ```bash
   cd "Improving Deep Neural Networks Hyperparameter Tuning, Regularization and Optimization (Module 2)"
   ```
3. Open and run the Jupyter Notebooks in weekly order:
   ```bash
   jupyter notebook
   ```

## 📝 Key Implementations

### Regularization Techniques
- L2 regularization implementation
- Dropout forward and backward propagation
- Early stopping mechanisms

### Optimization Algorithms
- Mini-batch gradient descent
- Momentum optimization
- RMSProp optimizer
- Adam optimization algorithm

### Advanced Concepts
- Batch normalization for deep networks
- Hyperparameter tuning with systematic approaches
- TensorFlow implementation of neural networks

## 🔗 References
- [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning)
- [TensorFlow Documentation](https://www.tensorflow.org/api_docs)
- [Original Course Materials](https://www.coursera.org/learn/deep-neural-network)

---

**Happy Learning!** 🚀  
**Ahmed Buttar**  
📧 [Email](mailto:muhanmadahmed02@gmail.com) | 🔗 [GitHub](https://github.com/AhmedButtar7) | 💼 [LinkedIn]([https://linkedin.com/in/ahmedbuttar](https://www.linkedin.com/in/muhammad-ahmed-a29142349?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app))
